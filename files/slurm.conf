ClusterName=cloudbarkla.alces.network
ControlMachine=cadmin01
SlurmUser=nobody
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
SlurmdSpoolDir=/var/spool/slurmd.spool
StateSaveLocation=/var/spool/slurm.state
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# Scheduler parameters
SchedulerType=sched/backfill
#SelectType=select/linear
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory

GresTypes=gpu

FastSchedule=1
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none

# Enable memory limits
DefMemPerCPU=500
DisableRootJobs=true

# Node configurations
NodeName=cnode[01-02] Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 RealMemory=14500 State=UNKNOWN Weight=10
NodeName=cnode[03-23] Sockets=1 CoresPerSocket=36 ThreadsPerCore=2 RealMemory=110000 State=UNKNOWN Weight=5
NodeName=cgpu01 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN RealMemory=16000 Gres=gpu:1 Weight=30

# Partitions
PartitionName=nodes Nodes=cnode[01-23] Default=YES MaxTime=UNLIMITED PriorityTier=1000 DefMemPerCPU=500 OverSubscribe=NO
PartitionName=gpu Nodes=cgpu01 Default=NO MaxTime=UNLIMITED PriorityTier=2000 DefMemPerCPU=1000 OverSubscribe=NO

